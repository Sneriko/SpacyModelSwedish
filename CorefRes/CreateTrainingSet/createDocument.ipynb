{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "import neuralcoref.train.document as dc\n",
    "from xml.etree import ElementTree\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "with open('pickleObjects/corefSpansObject', 'rb') as f:\n",
    "    corefSpansDocument = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "sweParts = []\n",
    "sweCorp = ''\n",
    "with open('../Data/Datasets/Europarl/Documents/ep-11-06-23-004/text/ep-11-06-23-004-sv.txt') as f:\n",
    "    for index, line in enumerate(f):\n",
    "        sweCorp = sweCorp + line\n",
    "        if (index + 1) % 18 == 0:\n",
    "            sweParts.append(sweCorp)\n",
    "            sweCorp = ''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "nlp = spacy.load('../Models/SwedishModel')\n",
    "docs = []\n",
    "for part in sweParts:\n",
    "    doc = nlp(part)\n",
    "    docs.append(doc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "corefSpansSweDocument = []\n",
    "\n",
    "for corefSpans, doc in zip(corefSpansDocument, docs):\n",
    "    numOfSingCoref = 0\n",
    "    corefSpansSwe = []\n",
    "    for clusterSub in corefSpans:\n",
    "        corefClusterSwe = []\n",
    "\n",
    "        for ment in clusterSub:\n",
    "            corefSwe = []\n",
    "            startOfLine = 0\n",
    "            endOfLine = 0\n",
    "            lineText = ''\n",
    "            mentionText = ''\n",
    "            if ment[0] != -1:\n",
    "                startOfLine = ment[0]\n",
    "                endOfLine = ment[-1]\n",
    "                while doc[startOfLine].text != '\\n' and startOfLine != 0:\n",
    "                    startOfLine -= 1\n",
    "                startOfLine += 1\n",
    "                while doc[endOfLine].text != '\\n':\n",
    "                    endOfLine += 1\n",
    "\n",
    "                for tok in range(startOfLine, endOfLine):\n",
    "                    lineText = lineText + doc[tok].text + ' '\n",
    "                lineText.rstrip()\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            #mentionSpanDoc = nlp(mentionText)\n",
    "            lineDoc = nlp(lineText)\n",
    "\n",
    "            #mentionsInSpan = dc.extract_mentions_spans(mentionSpanDoc, [])\n",
    "            mentionsInLine = dc.extract_mentions_spans(lineDoc, [])\n",
    "\n",
    "            mentionsInSpan = []\n",
    "\n",
    "            for span in mentionsInLine:\n",
    "                if (span.start + startOfLine) >= ment[0] and (span.end + startOfLine) <= ment[-1]:\n",
    "                    mentionsInSpan.append(span)\n",
    "\n",
    "            if len(mentionsInSpan) == 0:\n",
    "                continue\n",
    "\n",
    "            invalidCoref = False\n",
    "\n",
    "            \"\"\"for span in mentionsInLine:\n",
    "                if (span.end + startOfLine) > mention[1] > (span.start + startOfLine) > mention[0] or (span.start + startOfLine) < mention[0] < (span.end + startOfLine) < mention[1]:\n",
    "                    invalidCoref = True\"\"\"\n",
    "\n",
    "            maxLength = 0\n",
    "            maxSpan = 0\n",
    "            for subSpan in mentionsInSpan:\n",
    "                if subSpan.end - subSpan.start > maxLength:\n",
    "                    maxLength = subSpan.end - subSpan.start\n",
    "                    maxSpan = subSpan\n",
    "\n",
    "            \"\"\"for span in mentionsInSpan:\n",
    "                if span.start < maxSpan.start or span.end > maxSpan.end:\n",
    "                    invalidCoref = True\"\"\"\n",
    "\n",
    "            if invalidCoref:\n",
    "                continue\n",
    "            elif not invalidCoref:\n",
    "                corefSwe.append(maxSpan.start + startOfLine)\n",
    "                corefSwe.append(maxSpan.end + startOfLine)\n",
    "                corefClusterSwe.append(corefSwe)\n",
    "\n",
    "        if len(corefClusterSwe) > 1:\n",
    "            res = list(set(tuple(sorted(sub)) for sub in corefClusterSwe))\n",
    "            corefSpansSwe.append(res)\n",
    "        else:\n",
    "            numOfSingCoref += 1\n",
    "\n",
    "    corefSpansSweDocument.append(corefSpansSwe)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "dictSpeakerSents = {}\n",
    "tree = ElementTree.parse('../Data/Datasets/Europarl/Documents/ep-11-06-23-004/xml/ep-11-06-23-004-sv.xml')\n",
    "root = tree.getroot()\n",
    "for speaker in root.iter('SPEAKER'):\n",
    "    for sent in speaker.iter('s'):\n",
    "        dictSpeakerSents[sent.attrib['id']] = speaker.attrib['NAME'].replace(' ', '_')\n",
    "\n",
    "dictSpeakerLines = {}\n",
    "tree = ElementTree.parse('../Data/Datasets/Europarl/Documents/ep-11-06-23-004/xml/ep-11-06-23-004-links.xml')\n",
    "root = tree.getroot()\n",
    "lineNumber = 0\n",
    "\n",
    "for link in root.iter('link'):\n",
    "    sweLinks = link.attrib['xtargets'].split(';')[1].split()\n",
    "    noSpeakerInLine = False\n",
    "\n",
    "    for sentNumber in sweLinks:\n",
    "        if sentNumber not in dictSpeakerSents:\n",
    "            noSpeakerInLine = True\n",
    "\n",
    "    for sentNumber in sweLinks:\n",
    "        if not noSpeakerInLine:\n",
    "            dictSpeakerLines[lineNumber] = dictSpeakerSents[sentNumber]\n",
    "\n",
    "    lineNumber += 1\n",
    "\n",
    "dictSpeakerTokensDocument = []\n",
    "for index1, swePart in enumerate(sweParts):\n",
    "    dictSpeakerTokens = {}\n",
    "    tokenIndex = 0\n",
    "\n",
    "    sweLines = swePart.split('\\n')\n",
    "    for index2, line in enumerate(sweLines):\n",
    "        sweLines[index2] = line + '\\n'\n",
    "    sweLines.pop()\n",
    "\n",
    "    for lineIndex, line in enumerate(sweLines):\n",
    "        lineDoc = nlp(line)\n",
    "        key = index1 * 18 + lineIndex\n",
    "        if key in dictSpeakerLines:\n",
    "            for i, token in enumerate(lineDoc):\n",
    "                dictSpeakerTokens[tokenIndex + i] = dictSpeakerLines[key]\n",
    "        else:\n",
    "            for i, token in enumerate(lineDoc):\n",
    "                dictSpeakerTokens[tokenIndex + i] = None\n",
    "        tokenIndex += len(lineDoc)\n",
    "\n",
    "    dictSpeakerTokensDocument.append(dictSpeakerTokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "corefDictsDocument = []\n",
    "\n",
    "for corefSpans in corefSpansSweDocument:\n",
    "    corefClusterId = 1\n",
    "    corefDict = {}\n",
    "    for cluster in corefSpans:\n",
    "        for span in cluster:\n",
    "            if span[1] - span[0] == 1:\n",
    "                key = str(span[0])\n",
    "                if corefDict.get(key) is not None:\n",
    "                    corefDict[key] = corefDict[key] + '|(' + str(corefClusterId) + ')'\n",
    "                else:\n",
    "                    corefDict[key] = '(' + str(corefClusterId) + ')'\n",
    "            else:\n",
    "                keyStart = str(span[0])\n",
    "                keyEnd = str(span[1] - 1)\n",
    "                if corefDict.get(keyStart) is not None:\n",
    "                    corefDict[keyStart] = corefDict[keyStart] + '|(' + str(corefClusterId)\n",
    "                else:\n",
    "                    corefDict[keyStart] = '(' + str(corefClusterId)\n",
    "                if corefDict.get(keyEnd) is not None:\n",
    "                    corefDict[keyEnd] = corefDict[keyEnd] + '|' + str(corefClusterId) + ')'\n",
    "                else:\n",
    "                    corefDict[keyEnd] = str(corefClusterId) + ')'\n",
    "        corefClusterId += 1\n",
    "    corefDictsDocument.append(corefDict)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "dataFramesDocument = []\n",
    "\n",
    "for i, (corefAnnot, speakersTok) in enumerate(zip(corefDictsDocument, dictSpeakerTokensDocument)):\n",
    "\n",
    "    df2 = pd.DataFrame({'DocName': 'ep-11-06-23-004',\n",
    "                    'Part': pd.Series(i, index = list(range(len(docs[i]))), dtype='int32'),\n",
    "                    'TokenID': 1,\n",
    "                    'Text': '',\n",
    "                    'Tag': '',\n",
    "                    'Speaker': '',\n",
    "                    'Corefs': '-',})\n",
    "    tokenIdDict = {}\n",
    "\n",
    "    for sent in docs[i].sents:\n",
    "        idTok = 0\n",
    "        for token in sent:\n",
    "            key = str(token.i)\n",
    "            tokenIdDict[key] = idTok\n",
    "            if token.text != '\\n':\n",
    "                idTok += 1\n",
    "\n",
    "    for j in range(len(docs[i])):\n",
    "        key = str(j)\n",
    "        df2.at[j, 'TokenID'] = tokenIdDict[key]\n",
    "\n",
    "    for j, token in enumerate(docs[i]):\n",
    "        df2.at[j, 'Text'] = token.text\n",
    "        df2.at[j, 'Tag'] = token.tag_\n",
    "\n",
    "    tokIndex = 0\n",
    "    tokStartIndex = 0\n",
    "    for sent in docs[i].sents:\n",
    "        for j, token in enumerate(sent):\n",
    "            if speakersTok.get(tokIndex - j) is not None:\n",
    "                df2.at[tokIndex, 'Speaker'] = speakersTok[tokIndex - j]\n",
    "            tokIndex += 1\n",
    "\n",
    "    for j in range(len(docs[i])):\n",
    "        key = str(j)\n",
    "        if corefAnnot.get(key) is not None:\n",
    "            df2.at[j, 'Corefs'] = corefAnnot[key]\n",
    "\n",
    "    df2 = df2[df2.Text != '\\n'].reset_index(drop=True)\n",
    "\n",
    "    dataFramesDocument.append(df2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "with open('conll/ep-11-06-23-004.v4_gold_conll', 'w') as file:\n",
    "    for df in dataFramesDocument:\n",
    "        file.write('#begin document (ep-11-06-23-004); part ' + str(df.iat[0, 1]) + '\\n')\n",
    "        for i in range(len(df)):\n",
    "            a = df.loc[i, 'TokenID']\n",
    "            if df.loc[i, 'TokenID'] == 0:\n",
    "                file.write('\\n')\n",
    "            file.write(df.loc[i, 'DocName'] + '\\t\\t\\t')\n",
    "            file.write(str(df.loc[i, 'Part']) + '\\t\\t\\t')\n",
    "            file.write(str(df.loc[i, 'TokenID']) + '\\t\\t\\t')\n",
    "            file.write('{:40s}'.format(df.loc[i, 'Text']))\n",
    "            file.write('{:40s}'.format(df.loc[i, 'Tag']))\n",
    "            file.write('-' + '\\t\\t\\t' + '-' + '\\t\\t\\t' + '-' + '\\t\\t\\t' + '-' + '\\t\\t\\t')\n",
    "            file.write('{:40s}'.format(df.loc[i, 'Speaker']))\n",
    "            file.write('-' + '\\t\\t\\t')\n",
    "            file.write(df.loc[i, 'Corefs'])\n",
    "            file.write('\\n')\n",
    "\n",
    "        file.write('\\n' + '#end document' + '\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}