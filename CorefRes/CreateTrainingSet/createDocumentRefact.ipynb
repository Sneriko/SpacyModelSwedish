{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "import neuralcoref.train.document as dc\n",
    "from xml.etree import ElementTree\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from Coref.KBBert.SweBertNer import SweBertNer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loads the Swedish coref spans"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('pickleObjects/corefSpansObject', 'rb') as f:\n",
    "    corefSpansDocument = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read the Swedish documents into parts of 20 lines each"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def readIntoParts(textFile):\n",
    "    sweParts = []\n",
    "    sweCorp = ''\n",
    "    with open(textFile) as f:\n",
    "        for index, line in enumerate(f):\n",
    "            sweCorp = sweCorp + line\n",
    "            if (index + 1) % 20 == 0:\n",
    "                sweParts.append(sweCorp)\n",
    "                sweCorp = ''\n",
    "    return sweParts"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loads the Swedish Spacy model with the transormer Swedish Bert ner"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def loadModel(model):\n",
    "    nlp = spacy.load(model)\n",
    "    nlpTrans = pipeline('ner', model='KB/bert-base-swedish-cased-neriob', tokenizer='KB/bert-base-swedish-cased-neriob')\n",
    "    nlp.remove_pipe('ner')\n",
    "\n",
    "    trans = SweBertNer(nlpTrans, nlp)\n",
    "    nlp.add_pipe(trans)\n",
    "    return nlp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Process the Swedish parts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def processDocs(nlp, sweParts):\n",
    "    docs = []\n",
    "    for part in sweParts:\n",
    "        doc = nlp(part)\n",
    "        docs.append(doc)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Translates the Swedish coref spans to valid coref annotations in the Swedish\n",
    "documents by extracting the max mention from the spans."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def translateCorefs(nlp, pdocs):\n",
    "\n",
    "    corefSpansSweDocument = []\n",
    "\n",
    "    for corefSpans, doc in zip(corefSpansDocument, pdocs):\n",
    "        numOfSingCoref = 0\n",
    "        corefSpansSwe = []\n",
    "        for clusterSub in corefSpans:\n",
    "            corefClusterSwe = []\n",
    "\n",
    "            for ment in clusterSub:\n",
    "                corefSwe = []\n",
    "                startOfLine = 0\n",
    "                endOfLine = 0\n",
    "                lineText = ''\n",
    "                mentionText = ''\n",
    "                if ment[0] != -1:\n",
    "                    startOfLine = ment[0]\n",
    "                    endOfLine = ment[-1]\n",
    "                    while doc[startOfLine].text != '\\n' and startOfLine != 0:\n",
    "                        startOfLine -= 1\n",
    "                    startOfLine += 1\n",
    "                    while doc[endOfLine].text != '\\n':\n",
    "                        endOfLine += 1\n",
    "\n",
    "                    for tok in range(startOfLine, endOfLine):\n",
    "                        lineText = lineText + doc[tok].text + ' '\n",
    "                    lineText.rstrip()\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                #mentionSpanDoc = nlp(mentionText)\n",
    "                lineDoc = nlp(lineText)\n",
    "\n",
    "                #mentionsInSpan = dc.extract_mentions_spans(mentionSpanDoc, [])\n",
    "                mentionsInLine = dc.extract_mentions_spans(lineDoc, [])\n",
    "\n",
    "                mentionsInSpan = []\n",
    "\n",
    "                for span in mentionsInLine:\n",
    "                    if (span.start + startOfLine) >= ment[0] and (span.end + startOfLine) <= ment[-1]:\n",
    "                        mentionsInSpan.append(span)\n",
    "\n",
    "                if len(mentionsInSpan) == 0:\n",
    "                    continue\n",
    "\n",
    "                invalidCoref = False\n",
    "\n",
    "                \"\"\"for span in mentionsInLine:\n",
    "                    if (span.end + startOfLine) > mention[1] > (span.start + startOfLine) > mention[0] or (span.start + startOfLine) < mention[0] < (span.end + startOfLine) < mention[1]:\n",
    "                        invalidCoref = True\"\"\"\n",
    "\n",
    "                maxLength = 0\n",
    "                maxSpan = 0\n",
    "                for subSpan in mentionsInSpan:\n",
    "                    if subSpan.end - subSpan.start > maxLength:\n",
    "                        maxLength = subSpan.end - subSpan.start\n",
    "                        maxSpan = subSpan\n",
    "\n",
    "                \"\"\"for span in mentionsInSpan:\n",
    "                    if span.start < maxSpan.start or span.end > maxSpan.end:\n",
    "                        invalidCoref = True\"\"\"\n",
    "\n",
    "                if invalidCoref:\n",
    "                    continue\n",
    "                elif not invalidCoref:\n",
    "                    corefSwe.append(maxSpan.start + startOfLine)\n",
    "                    corefSwe.append(maxSpan.end + startOfLine)\n",
    "                    corefClusterSwe.append(corefSwe)\n",
    "\n",
    "            if len(corefClusterSwe) > 1:\n",
    "                res = list(set(tuple(sorted(sub)) for sub in corefClusterSwe))\n",
    "                corefSpansSwe.append(res)\n",
    "            else:\n",
    "                numOfSingCoref += 1\n",
    "\n",
    "        corefSpansSweDocument.append(corefSpansSwe)\n",
    "    return corefSpansSweDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract speakers for each token in the documents"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extractSpeakers(tokenizer, sweParts, sweDocXml, linksXml):\n",
    "\n",
    "    dictSpeakerSents = {}\n",
    "    tree = ElementTree.parse(sweDocXml)\n",
    "    root = tree.getroot()\n",
    "    for speaker in root.iter('SPEAKER'):\n",
    "        for sent in speaker.iter('s'):\n",
    "            dictSpeakerSents[sent.attrib['id']] = speaker.attrib['NAME'].replace(' ', '_')\n",
    "\n",
    "    dictSpeakerLines = {}\n",
    "    tree = ElementTree.parse(linksXml)\n",
    "    root = tree.getroot()\n",
    "    lineNumber = 0\n",
    "\n",
    "    for link in root.iter('link'):\n",
    "        sweLinks = link.attrib['xtargets'].split(';')[1].split()\n",
    "        noSpeakerInLine = False\n",
    "\n",
    "        for sentNumber in sweLinks:\n",
    "            if sentNumber not in dictSpeakerSents:\n",
    "                noSpeakerInLine = True\n",
    "\n",
    "        for sentNumber in sweLinks:\n",
    "            if not noSpeakerInLine:\n",
    "                dictSpeakerLines[lineNumber] = dictSpeakerSents[sentNumber]\n",
    "\n",
    "        lineNumber += 1\n",
    "\n",
    "    dictSpeakerTokensDocument = []\n",
    "    for index1, swePart in enumerate(sweParts):\n",
    "        dictSpeakerTokens = {}\n",
    "        tokenIndex = 0\n",
    "\n",
    "        sweLines = swePart.split('\\n')\n",
    "        for index2, line in enumerate(sweLines):\n",
    "            sweLines[index2] = line + '\\n'\n",
    "        sweLines.pop()\n",
    "\n",
    "        for lineIndex, line in enumerate(sweLines):\n",
    "            lineDoc = tokenizer(line)\n",
    "            key = index1 * 18 + lineIndex\n",
    "            if key in dictSpeakerLines:\n",
    "                for i, token in enumerate(lineDoc):\n",
    "                    dictSpeakerTokens[tokenIndex + i] = dictSpeakerLines[key]\n",
    "            else:\n",
    "                for i, token in enumerate(lineDoc):\n",
    "                    dictSpeakerTokens[tokenIndex + i] = None\n",
    "            tokenIndex += len(lineDoc)\n",
    "\n",
    "        dictSpeakerTokensDocument.append(dictSpeakerTokens)\n",
    "\n",
    "    return dictSpeakerTokensDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creates a coref annotation dict to be stored in the dataframes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def createCorefAnnotDict(corefSpansSweDoc):\n",
    "\n",
    "    corefDictsDocument = []\n",
    "\n",
    "    for corefSpans in corefSpansSweDoc:\n",
    "        corefClusterId = 1\n",
    "        corefDict = {}\n",
    "        for clust in corefSpans:\n",
    "            for span in clust:\n",
    "                if span[1] - span[0] == 1:\n",
    "                    key = str(span[0])\n",
    "                    if corefDict.get(key) is not None:\n",
    "                        corefDict[key] = corefDict[key] + '|(' + str(corefClusterId) + ')'\n",
    "                    else:\n",
    "                        corefDict[key] = '(' + str(corefClusterId) + ')'\n",
    "                else:\n",
    "                    keyStart = str(span[0])\n",
    "                    keyEnd = str(span[1] - 1)\n",
    "                    if corefDict.get(keyStart) is not None:\n",
    "                        corefDict[keyStart] = corefDict[keyStart] + '|(' + str(corefClusterId)\n",
    "                    else:\n",
    "                        corefDict[keyStart] = '(' + str(corefClusterId)\n",
    "                    if corefDict.get(keyEnd) is not None:\n",
    "                        corefDict[keyEnd] = corefDict[keyEnd] + '|' + str(corefClusterId) + ')'\n",
    "                    else:\n",
    "                        corefDict[keyEnd] = str(corefClusterId) + ')'\n",
    "            corefClusterId += 1\n",
    "        corefDictsDocument.append(corefDict)\n",
    "    return corefDictsDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creates dataframes with coref annotations, speakers, tokenID:s and partID:s\n",
    "for each of the document parts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def createDataframes(pdocs, corefAnnotDicts, dictSpeakerDocs):\n",
    "\n",
    "    dataFramesDocument = []\n",
    "\n",
    "    for i, (corefAnnot, speakersTok) in enumerate(zip(corefAnnotDicts, dictSpeakerDocs)):\n",
    "\n",
    "        df2 = pd.DataFrame({'DocName': 'ep-07-05-23-006',\n",
    "                        'Part': pd.Series(i, index = list(range(len(pdocs[i]))), dtype='int32'),\n",
    "                        'TokenID': 1,\n",
    "                        'Text': '',\n",
    "                        'Tag': '',\n",
    "                        'Speaker': '',\n",
    "                        'Corefs': '-',})\n",
    "        tokenIdDict = {}\n",
    "\n",
    "        for sent in pdocs[i].sents:\n",
    "            idTok = 0\n",
    "            for token in sent:\n",
    "                key = str(token.i)\n",
    "                tokenIdDict[key] = idTok\n",
    "                if token.text != '\\n':\n",
    "                    idTok += 1\n",
    "\n",
    "        for j in range(len(pdocs[i])):\n",
    "            key = str(j)\n",
    "            df2.at[j, 'TokenID'] = tokenIdDict[key]\n",
    "\n",
    "        for j, token in enumerate(pdocs[i]):\n",
    "            df2.at[j, 'Text'] = token.text\n",
    "            df2.at[j, 'Tag'] = token.tag_\n",
    "\n",
    "        tokIndex = 0\n",
    "        tokStartIndex = 0\n",
    "        for sent in pdocs[i].sents:\n",
    "            for j, token in enumerate(sent):\n",
    "                if speakersTok.get(tokIndex - j) is not None:\n",
    "                    df2.at[tokIndex, 'Speaker'] = speakersTok[tokIndex - j]\n",
    "                tokIndex += 1\n",
    "\n",
    "        for j in range(len(pdocs[i])):\n",
    "            key = str(j)\n",
    "            if corefAnnot.get(key) is not None:\n",
    "                df2.at[j, 'Corefs'] = corefAnnot[key]\n",
    "\n",
    "        df2 = df2[df2.Text != '\\n'].reset_index(drop=True)\n",
    "\n",
    "        dataFramesDocument.append(df2)\n",
    "    return dataFramesDocument"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "sweParts = readIntoParts('../Data/Datasets/Europarl/Documents/ep-07-05-23-006/text/ep-07-05-23-006-sv.txt')\n",
    "\n",
    "tokenizer = spacy.load('../Models/SwedishModel', disable=['tagger', 'parser', 'ner'])\n",
    "nlp = loadModel('../Models/SwedishModel')\n",
    "\n",
    "docs = processDocs(nlp, sweParts)\n",
    "\n",
    "corefSpansSweDocument = translateCorefs(nlp, docs)\n",
    "\n",
    "dictSpeakerTokensDoc = extractSpeakers(tokenizer, sweParts, '../Data/Datasets/Europarl/Documents/ep-07-05-23-006/xml/ep-07-05-23-006-sv.xml', '../Data/Datasets/Europarl/Documents/ep-07-05-23-006/xml/ep-07-05-23-006-links.xml')\n",
    "\n",
    "corefAnnotDict = createCorefAnnotDict(corefSpansSweDocument)\n",
    "\n",
    "dataframes = createDataframes(docs, corefAnnotDict, dictSpeakerTokensDoc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Writes the dataframes to a single v4.gold.conll file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "with open('conll/ep-07-05-23-006-Allen.v4_gold_conll', 'w') as file:\n",
    "    for df in dataframes:\n",
    "        file.write('#begin document (ep-07-05-23-006); part ' + str(df.iat[0, 1]) + '\\n')\n",
    "        for i in range(len(df)):\n",
    "            a = df.loc[i, 'TokenID']\n",
    "            if df.loc[i, 'TokenID'] == 0:\n",
    "                file.write('\\n')\n",
    "            file.write(df.loc[i, 'DocName'] + '\\t\\t\\t')\n",
    "            file.write(str(df.loc[i, 'Part']) + '\\t\\t\\t')\n",
    "            file.write(str(df.loc[i, 'TokenID']) + '\\t\\t\\t')\n",
    "            file.write('{:40s}'.format(df.loc[i, 'Text']))\n",
    "            file.write('{:40s}'.format(df.loc[i, 'Tag']))\n",
    "            file.write('-' + '\\t\\t\\t' + '-' + '\\t\\t\\t' + '-' + '\\t\\t\\t' + '-' + '\\t\\t\\t')\n",
    "            file.write('{:40s}'.format(df.loc[i, 'Speaker']))\n",
    "            file.write('-' + '\\t\\t\\t')\n",
    "            file.write(df.loc[i, 'Corefs'])\n",
    "            file.write('\\n')\n",
    "\n",
    "        file.write('\\n' + '#end document' + '\\n')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Statistics for how many coref-annotations were translated from the original\n",
    "English document."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "numOfCorefSwe = 0\n",
    "\n",
    "for dok in corefSpansSweDocument:\n",
    "    for cluster in dok:\n",
    "        for coref in cluster:\n",
    "            numOfCorefSwe +=1\n",
    "\n",
    "numOfCorefEng = 0\n",
    "\n",
    "for dok in corefSpansDocument:\n",
    "    for cluster in dok:\n",
    "        for coref in cluster:\n",
    "            numOfCorefEng +=1\n",
    "\n",
    "numberOfNoAli = 0\n",
    "\n",
    "for dok in corefSpansDocument:\n",
    "    for cluster in dok:\n",
    "        for coref in cluster:\n",
    "            if len(cluster) == 2 and coref[0] == -1:\n",
    "                numberOfNoAli += 2\n",
    "            elif coref[0] == -1:\n",
    "                numberOfNoAli +=1\n",
    "\n",
    "transCorefs = numOfCorefSwe / numOfCorefEng\n",
    "transCorefInclAliErr = numOfCorefSwe / numberOfNoAli"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}